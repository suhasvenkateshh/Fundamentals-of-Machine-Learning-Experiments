import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from mlxtend.plotting import plot_decision_regions

# Step 1: Create dataset
df = pd.DataFrame()
df['X1'] = [1, 2, 3, 4, 5, 6, 6, 7, 9, 9]
df['X2'] = [5, 3, 6, 8, 1, 9, 5, 8, 9, 2]
df['label'] = [1, 1, 0, 1, 0, 1, 0, 1, 0, 0]
df['weights'] = 1 / df.shape[0]

# ðŸŽ¯ Visualize initial data
plt.figure(figsize=(6, 5))
sns.scatterplot(x='X1', y='X2', hue='label', data=df, palette='Set1', s=80)
plt.title("Initial Data Distribution")
plt.grid(True)
plt.show()

# Step 2: Train first decision stump
x = df[['X1', 'X2']].values
y = df['label'].values
dt1 = DecisionTreeClassifier(max_depth=1)
dt1.fit(x, y)
df['y_pred'] = dt1.predict(x)

# ðŸ“Š Plot decision region - Tree 1
plt.figure(figsize=(6, 5))
plot_decision_regions(x, y, clf=dt1, legend=2)
plt.title("Decision Region - Tree 1")
plt.grid(True)
plt.show()

# Step 3: Calculate model weight
def calculate_model_weight(error):
    return 0.5 * np.log((1 - error) / error)

error1 = np.sum(df['weights'] * (df['label'] != df['y_pred']))
alpha1 = calculate_model_weight(error1)

# Step 4: Update weights
def update_row_weights(row, alpha):
    if row['label'] == row['y_pred']:
        return row['weights'] * np.exp(-alpha)
    else:
        return row['weights'] * np.exp(alpha)

df['updated_weights'] = df.apply(update_row_weights, axis=1, alpha=alpha1)
df['normalized_weights'] = df['updated_weights'] / df['updated_weights'].sum()
df['cumsum_upper'] = np.cumsum(df['normalized_weights'])
df['cumsum_lower'] = df['cumsum_upper'] - df['normalized_weights']

# Step 5: Resample based on weights
def create_new_dataset(df):
    indices = []
    for _ in range(df.shape[0]):
        a = np.random.random()
        for index, row in df.iterrows():
            if row['cumsum_lower'] < a <= row['cumsum_upper']:
                indices.append(index)
                break
    return indices

index_values = create_new_dataset(df)
second_df = df.iloc[index_values].copy()

# Step 6: Train second decision stump
x2 = second_df[['X1', 'X2']].values
y2 = second_df['label'].values
dt2 = DecisionTreeClassifier(max_depth=1)
dt2.fit(x2, y2)
second_df['y_pred'] = dt2.predict(x2)

# ðŸ“Š Plot decision region - Tree 2
plt.figure(figsize=(6, 5))
plot_decision_regions(x2, y2, clf=dt2, legend=2)
plt.title("Decision Region - Tree 2")
plt.grid(True)
plt.show()

# Step 7: Calculate second model weight
error2 = np.sum(second_df['weights'] * (second_df['label'] != second_df['y_pred']))
alpha2 = calculate_model_weight(error2)

# Step 8: Train third decision stump on original data
dt3 = DecisionTreeClassifier(max_depth=1)
dt3.fit(x, y)
df['y_pred_3'] = dt3.predict(x)
error3 = np.sum(df['weights'] * (df['label'] != df['y_pred_3']))
alpha3 = calculate_model_weight(error3)

# ðŸ“Œ Print model weights
print("Alpha1:", round(alpha1, 3))
print("Alpha2:", round(alpha2, 3))
print("Alpha3:", round(alpha3, 3))

# Step 9: Ensemble prediction for query point
def ensemble_predict(query):
    pred1 = dt1.predict(query)[0]
    pred2 = dt2.predict(query)[0]
    pred3 = dt3.predict(query)[0]
    vote = alpha1 * (1 if pred1 == 1 else -1) + \
           alpha2 * (1 if pred2 == 1 else -1) + \
           alpha3 * (1 if pred3 == 1 else -1)
    return int(np.sign(vote) > 0)

# ðŸ” Test queries
query1 = np.array([1, 5]).reshape(1, -1)
query2 = np.array([9, 9]).reshape(1, -1)

print("Ensemble prediction for [1,5]:", ensemble_predict(query1))
print("Ensemble prediction for [9,9]:", ensemble_predict(query2))

###code for 8b

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.tree import DecisionTreeRegressor, plot_tree

# Set seed for reproducibility
np.random.seed(42)

# Generate synthetic data
X = np.random.rand(100, 1) - 0.5
y = 3 * X[:, 0]**2 + 0.05 * np.random.randn(100)

# ---------------- Polynomial Gradient Boosting ----------------
n_estimators = 5
learning_rate = 1.0
prediction = np.zeros_like(y)
intermediate_preds = []

for i in range(n_estimators):
    residual = y - prediction
    coeffs = np.polyfit(X[:, 0], residual, deg=2)
    model_pred = np.polyval(coeffs, X[:, 0])
    prediction += learning_rate * model_pred
    intermediate_preds.append(prediction.copy())

# Plotting: 4 subplots for polynomial boosting
fig, axs = plt.subplots(2, 2, figsize=(12, 8))
for idx, ax in enumerate(axs.flat):
    for i in range(n_estimators - 1):
        ax.plot(X[:, 0], intermediate_preds[i], 'r', linewidth=1)
    ax.plot(X[:, 0], intermediate_preds[-1], 'b', linewidth=2)
    ax.set_title(f'Poly Boosting Graph {idx + 1}')
    ax.set_xlim(-0.6, 0.6)
    ax.set_ylim(0.0, 0.8)
    ax.grid(True)

plt.tight_layout()
plt.suptitle("Polynomial Gradient Boosting - 4 Graphs", fontsize=16, y=1.03)
plt.show()

# ---------------- Decision Tree Gradient Boosting ----------------
df = pd.DataFrame({'X': X.reshape(100), 'y': y})

# Plot original data
plt.scatter(df['X'], df['y'])
plt.title('X vs y')
plt.xlabel('X')
plt.ylabel('y')
plt.show()

# Initial prediction: mean of y
df['pred1'] = df['y'].mean()
df['res1'] = df['y'] - df['pred1']

# Plot residuals
plt.scatter(df['X'], df['y'], label='Actual y')
plt.plot(df['X'], df['pred1'], color='red', label='Initial Prediction')
plt.title('Initial Prediction vs Actual')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()

# Fit first decision tree on residuals
tree1 = DecisionTreeRegressor(max_leaf_nodes=8, random_state=42)
tree1.fit(df[['X']], df['res1'])

# Visualize the tree
plt.figure(figsize=(10, 6))
plot_tree(tree1, filled=True, feature_names=['X'])
plt.title('First Decision Tree')
plt.show()

# Update predictions
df['pred2'] = df['pred1'] + tree1.predict(df[['X']])
df['res2'] = df['y'] - df['pred2']

# Recursive Gradient Boosting with Decision Trees
def gradient_boost(X, y, number, lr, count=1, regs=None, foo=None):
    if regs is None:
        regs = []

    if number == 0:
        return

    if count > 1:
        y = y - lr * regs[-1].predict(X)
    else:
        foo = y

    tree_reg = DecisionTreeRegressor(max_depth=5, random_state=42)
    tree_reg.fit(X, y)
    regs.append(tree_reg)

    x1 = np.linspace(-0.5, 0.5, 500).reshape(-1, 1)
    y_pred = sum(lr * reg.predict(x1) for reg in regs)

    print(f"Boosting Round: {count}")
    plt.figure()
    plt.plot(x1, y_pred, label='Boosted Prediction', linewidth=2)
    plt.scatter(X[:, 0], foo, color='red', label='True y', alpha=0.6)
    plt.title(f'Decision Tree Boosting Round {count}')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.legend()
    plt.show()

    gradient_boost(X, y, number - 1, lr, count + 1, regs, foo=foo)

# Run Decision Tree Gradient Boosting
gradient_boost(X, y, number=5, lr=1)
